{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Text classification from scratch.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMH/ZtX4a3gkgKtXvsKHFQl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ithingv/nlp_study/blob/main/Text_classification_from_scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xymkFML0Jq2d"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load the data: IMDB movie review sentiment classification"
      ],
      "metadata": {
        "id": "vcHTOu9sLrkq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !curl -O https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
        "# !tar -xf aclImdb_v1.tar.gz"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T4GDP1IwLjqw",
        "outputId": "48e45fdb-8bb1-45ac-badc-f87b7d01c459"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 80.2M  100 80.2M    0     0  9390k      0  0:00:08  0:00:08 --:--:-- 17.4M\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !ls aclImdb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rDlqrZY9LqcA",
        "outputId": "cdd99636-1a8e-4582-a776-24a4f074c1a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "imdbEr.txt  imdb.vocab\tREADME\ttest  train\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !ls aclImdb/test"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Usqk084zT_Ig",
        "outputId": "07a408c4-fab6-4152-ea72-6a1c90485dfa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "labeledBow.feat  neg  pos  urls_neg.txt  urls_pos.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !ls aclImdb/train"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "obNhtFV5UMeJ",
        "outputId": "5408ab16-464d-4e5a-886f-abfab3c59a05"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "labeledBow.feat  pos\tunsupBow.feat  urls_pos.txt\n",
            "neg\t\t unsup\turls_neg.txt   urls_unsup.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !cat aclImdb/train/pos/6248_7.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xPYqMuJ_UMgk",
        "outputId": "ddc47c03-5359-452d-d7f3-959b392b6d8f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Being an Austrian myself this has been a straight knock in my face. Fortunately I don't live nowhere near the place where this movie takes place but unfortunately it portrays everything that the rest of Austria hates about Viennese people (or people close to that region). And it is very easy to read that this is exactly the directors intention: to let your head sink into your hands and say \"Oh my god, how can THAT be possible!\". No, not with me, the (in my opinion) totally exaggerated uncensored swinger club scene is not necessary, I watch porn, sure, but in this context I was rather disgusted than put in the right context.<br /><br />This movie tells a story about how misled people who suffer from lack of education or bad company try to survive and live in a world of redundancy and boring horizons. A girl who is treated like a whore by her super-jealous boyfriend (and still keeps coming back), a female teacher who discovers her masochism by putting the life of her super-cruel \"lover\" on the line, an old couple who has an almost mathematical daily cycle (she is the \"official replacement\" of his ex wife), a couple that has just divorced and has the ex husband suffer under the acts of his former wife obviously having a relationship with her masseuse and finally a crazy hitchhiker who asks her drivers the most unusual questions and stretches their nerves by just being super-annoying.<br /><br />After having seen it you feel almost nothing. You're not even shocked, sad, depressed or feel like doing anything... Maybe that's why I gave it 7 points, it made me react in a way I never reacted before. If that's good or bad is up to you!"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32\n",
        "raw_train_ds = tf.keras.preprocessing.text_dataset_from_directory(\n",
        "    \"aclImdb/train\",\n",
        "    batch_size=batch_size,\n",
        "    validation_split=0.2,\n",
        "    subset=\"training\",\n",
        "    seed=2020,\n",
        ")\n",
        "raw_val_ds = tf.keras.preprocessing.text_dataset_from_directory(\n",
        "    \"aclImdb/train\",\n",
        "    batch_size=batch_size,\n",
        "    validation_split=0.2,\n",
        "    subset=\"validation\",\n",
        "    seed=2020,\n",
        ")\n",
        "raw_test_ds = tf.keras.preprocessing.text_dataset_from_directory(\n",
        "    \"aclImdb/test\", batch_size=batch_size\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G-7Fh13mUMlT",
        "outputId": "10e6569c-14c3-4ef6-b634-5a4b9c3dd9e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 25000 files belonging to 2 classes.\n",
            "Using 20000 files for training.\n",
            "Found 25000 files belonging to 2 classes.\n",
            "Using 5000 files for validation.\n",
            "Found 25000 files belonging to 2 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Number of batches in raw_train_ds: {raw_train_ds.cardinality()}\")\n",
        "print(f\"Number of batches in raw_val_ds: {raw_val_ds.cardinality()}\")\n",
        "print(f\"Number of batches in raw_test_ds: {raw_test_ds.cardinality()}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aVbfIKaCUMnz",
        "outputId": "4de491c0-088a-490c-a977-6d337cbdb5d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of batches in raw_train_ds: 625\n",
            "Number of batches in raw_val_ds: 157\n",
            "Number of batches in raw_test_ds: 782\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for text_batch, label_batch in raw_train_ds.take(1):\n",
        "    for i in range(5):\n",
        "        print(text_batch.numpy()[i])\n",
        "        print(label_batch.numpy()[i])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nAxNWC12UMqe",
        "outputId": "afe88213-bc11-4699-f610-3acbd0dbee7b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "b\"This is one of the best movies I've ever seen. It has very good acting by Hanks, Newman, and everyone else. Definitely Jude Law's best performance. The cinematography is excellent, the editing is about as good, and includes a great original score that really fits in with the mood of the movie. The production design is also a factor in what makes this movie special. To me, it takes a lot to beat Godfather, but the fantastic cinematography displayed wins this contest. Definitely a Best Picture nominee in my book.\"\n",
            "1\n",
            "b\"New York, I Love You, or rather should-be-titled Manhattan, I Love Looking At Your People In Sometimes Love, is a precise example of the difference between telling a story and telling a situation. Case in point, look at two of the segments in the film, one where Ethan Hawke lights a cigarette for a woman on a street and proceeds to chat her up with obnoxious sexy-talk, and another with Orlando Bloom trying to score a movie with an incredulous demand from his director to read two Dostoyevsky books. While the latter isn't a great story by any stretch, it's at least something that has a beginning, middle and end, as the composer tries to score, gets Dostoyevky dumped in his lap, and in the end gets some help (and maybe something more) from a girl he's been talking to as a liaison between him and the director. The Ethan Hawke scene, however, is like nothing, and feels like it, like a fluke added in or directed by a filmmaker phoning it in (or, for that matter, Hawke with a combo of Before Sunrise and Reality Bites).<br /><br />What's irksome about the film overall is seeing the few stories that do work really well go up against the one or two possible 'stories' and then the rest of the situations that unfold that are made to connect or overlap with one another (i.e. bits involving Bradley Cooper, Drea DeMatteo, Hayden Christensen, Andy Garcia, James Caan, Natalie Portman, etc). It's not even so much that the film- set practically always in *Manhattan* and not the New York of Queens or Staten Island or the Bronx (not even, say, Harlem or Washington Heights)- lacks a good deal of diversity, since there is some. It's the lack of imagination that one found in spades, for better or worse, in Paris J'taime. It's mostly got little to do with New York, except for passing references, and at its worst (the Julie Christie/Shia LaBeouf segment) it's incomprehensible on a level that is appalling.<br /><br />So, basically, wait for TV, and do your best to dip in and out of the film - in, that is, for three scenes: the aforementioned Bloom/Christina Ricci segment which is charming; the Brett Ratner directed segment (yes, X-Men 3 Brett Ratner) with a very funny story of a teen taking a girl in a wheelchair to the prom only to come upon a great big twist; and Eli Wallach and Cloris Leachman as an adorable quite old couple walking along in Brooklyn on their 67th wedding anniversary. Everything else can be missed, even Natalie Portman's directorial debut, and the return of a Hughes brother (only one, Allan) to the screen. A mixed bag is putting it lightly: it's like having to search through a bag of mixed nuts full of crappy peanuts to find the few almonds left.\"\n",
            "0\n",
            "b'Was convincing the world that he didn\\'t exist...<br /><br />This is a line that is probably remembered by a lot of people. It\\'s from The Usual Suspects of course in relation to Kaiser Gold..I mean Sose..<br /><br />I got another one like that: -The dumbest trick a director ever pulled was trying to convince an audience he actually had a storyline-<br /><br />This movie is one of the saddest pieces of film-making I have seen in a long time. It starts out so well, with really fantastic cinematography, great acting and a very smart premise. But alas, the only way this movie is heading is on a course of self-destruction. And it does so, not by a single blow but with nagging little wrist-cuts.<br /><br />Pay no attention to the comments here that marvel at the fact that they found a way to explain this donut. With enough booze in my brain I would probably be capable of explaining the very existence of mankind to a very plausible degree. I have seen and read about a dozen totally different ways people explained the story. And they vary from a story set totally in someones head, playing chess with himself, to a cunning way for a criminal to play out his enemies by means resembling chess gaming.<br /><br />And that\\'s all jolly swell. But at the same time it is a painful giveaway that there is something terribly wrong with this story. And apart from that, it is in any case a blunt rip off of a score of movies and books like \"Fight Club, Kill Bill, Casino, The Usual Suspects, Snatch, Magnolia and Shachnovelle. And we are not dealing with kind borrowing here, it\\'s a blatant robbery.<br /><br />What ultimately goes wrong here in this movie is that the storyline swirls like a drunk bum on speed. If this movie was a roller-coaster ride, you\\'d have crashed into the attraction next to it shorty after take off. There are so many twists in this movie which will never be resolved, that if it was a cocktail, you\\'d be needing a life supply of hurl-buckets to work of the nausea after drinking it. Nothing is ever explained and when you finally get some grasp of the direction you think it\\'s going, you get pulled in yet another one.<br /><br />I guess this story wasn\\'t going anywhere on paper and Ritchy must have thought that is was awesome to make a movie out of it anyway, being the next David Lynch or something.<br /><br />1/10 for totally violating one\\'s own work (Ritchy: seek professional help). What could have easily been a gem instead becomes a contrived art-piece, food for pseudo intellectuals to debate on at sundayafternoon debating-clubs. <br /><br />Spare your soul and stomach, avoid at all cost!'\n",
            "0\n",
            "b'When Sam Peckinpah\\'s superlative THE WILD BUNCH (1969) opened the door to outrageous displays of graphic cinematic ultra-violence, it did so with a talented (if whisky-marinated) hand guiding the camera and had a compelling story with characters who had actual depth, but in no time flat there were scores of imitators that fell far from the benchmark set by Peckinpah\\'s epic, and SOLDIER BLUE definitely falls into that category.<br /><br />SOLDIER BLEW, er, BLUE tells the story of foul-mouthed New Yorker Cresta Lee (Candice Bergen) a blonde proto-hippie chick who\\'s been \"rescued\" from two years of \"captivity\" among the Cheyenne and is now being sent to a fort where she\\'ll be reunited with the fianc\\xc3\\xa9e she only wants to marry for his money. Also on board the wagon she\\'s traveling in is a shipment of government gold, cash the Cheyenne need to buy guns with, so in short order the soldiers are wiped out and Cresta flees to the hills, accompanied by Honus Gant (Peter Strauss), the lone surviving cavalryman. Calling Gant by the snarky nickname \"Soldier Blue,\" Cresta demonstrates that her years among the \"savages\" was time well spent, outstripping Gant in survival skills, common sense, and sheer balls, and over their journey toward the fort they must persevere against the elements, a band of hostile Kiowa, an unscrupulous trader \\xc2\\x97 played by Donald Pleasance, here giving one of his most ridiculous performances, and that\\'s saying something \\xc2\\x97 and, in the tradition of many previous western-set romantic comedies, each other.<br /><br />During the course of their misadventures the two opposites are inevitably \\xc2\\x97 and predictably \\xc2\\x97 attracted to each other and eventually end up getting it on \\xc2\\x97 while Gant has a freshly- treated bullet wound that went clean through his leg, no less \\xc2\\x97 in what was surely the only conveniently located cave for at least a twelve mile radius that wasn\\'t filled with rattlesnakes, mountain lions, or who knows what, to say nothing of the Cheyenne, who could have done something really spiffy with such a primo apartment (there I go, thinking in NYC real estate terms again). <br /><br />Realizing that their love could never flourish outside of the cave, Cresta leaves Gant and makes it to the fort by herself only to discover that the moron in charge won\\'t spare a couple of men so they can rescue Gant; the regiment needs all available personnel to launch an attack on the nearby Cheyenne village, and once Cresta gets wind of that she slips past her obnoxiously horny hubby-to-be and makes a beeline straight to the Cheyenne to warn them of what\\'s coming. <br /><br />What happens next is what gained the film its infamy; it turns out that all the wacky misadventures and squabbling were all just a lead-in to a hideous reenactment of the 1864 Sand Creek Massacre, an orgy of rape, torture and general sadistic evil perpetrated in the name of \"keeping the country clean,\" and almost forty years after its release this sequence still disturbs and nauseates for its sheer cruelty. Children are trampled beneath the hooves of charging horses or impaled on bayonets, unarmed people are beheaded \\xc2\\x97 a nice effect, I have to admit \\xc2\\x97 women are stripped and pawed by gangs of slavering brutes, then raped and mutilated \\xc2\\x97 in one truly sickening instance a naked native woman puts up too much of a fight, so her rapist instead decides to cut off her breasts, which we thankfully only see the start of before the camera moves on to chronicle some other hideous act \\xc2\\x97 and scores of innocent people are shot and dismembered, their compone nt parts impaled on pikes and waved about in victorious celebration or kept as the most ghoulish of souvenirs. No joke, this scene would instantly garner an NC-17 rating if released today, to say nothing of possibly spurring Native American interest groups to riot in the streets over the incredibly exploitative manner in which the atrocities are depicted.<br /><br />I\\'m all in favor of westerns that don\\'t shy away from honest portrayals of how the west was won, or stolen if truth be told, but this film has no idea of what kind of movie it wants to be; one minute it\\'s a heavy-handed pseudo-hippy lecture about how the treatment of the natives was totally effed up (well, DUH!), then it\\'s a light-hearted battle of the sexes farce wherein Cresta proves herself five times the man Gant is and manages to look hot in her tasty red calico poncho (with no undies), but that all goes out the window when Donald Pleasance shows up with an unintentionally (?) hilarious pair of buck-toothed dentures and our heroes must figure out how to escape from his murderous clutches in a sub-plot that goes nowhere, all of which culminates in the aforementioned apocalyptic climax. Any one of those tacks would have been okay for a coherent film, but the end result is a slapdash mess that milked the horrors of its final ten minutes for all they were worth in the film\\'s promotion and poster imagery. <br /><br />But by trying to be all things to all audiences, SOLDIER BLUE ends up as an incoherent, preachy Mulligan stew of presumably well-intentioned political correctness, but if they were going to tell the story of the Sand Creek Massacre, wouldn\\'t it have been a good idea to have some Indian characters who were more than just walk-ons with Murphy Brown acting as their mouthpiece? We get to know absolutely nothing of the people who get wiped out solely for what appears to be a crass ploy to lure gorehound moviegoers into seeing \"the most savage film in history.\" If you, like me, were intrigued by the provocative ads and reviews that shower almost endless praise upon it for its \"daring to tell it like it was,\" take my word for it and let SOLDIER BLUE slowly fade into cinematic obscurity.'\n",
            "0\n",
            "b\"I have been waiting for such an original picture such as this for quite some time now. Brokedown Palace has that `hard to believe' aspect, however with the ingenious directing, and screenplay, \\xc2\\x91Palace scores big. <br /><br /> I've really never enjoyed watching Claire Danes in any of her movies, but I'll tell you what, she really changed my mind with this one. Kate Beckinsale joins Danes on a vacation to Thailand where they meet up with a young man who convinces them to take a trip to Hong Kong with him. However, he neglected to inform them that they would be carrying an obscene amount of narcotics for him. Well, ultimately they get caught, and end up in a Thailand prison. I know, I know, how could you not know that you have 18 pounds of drugs? Well if you can get by that one tiny miscue, you will find a very well written, and acted out story. Lately I have found myself getting drawn into the storylines of the movies I watch, and developing a personal feeling for the characters that I watch. This movie is no different. By the end of the movie I found myself caring more for her than any of the other characters in the film. <br /><br /> Bill Pullman plays Hank Green, an attorney who lives in Thailand and specializes in international relations. As always Pullman delivers with an excellent performance and ties the movie together beautifully. <br /><br /> There are a few twists and turns that will, by the end of the movie, have you in tears. This is one of those movies that you have most likely passed by when searching for that movie to watch at home. It is my opinion that the next time out at the video store, do not pass by this one again.\"\n",
            "1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prepare the data"
      ],
      "metadata": {
        "id": "liNusc7HWJrS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# remove <br /> tags\n",
        "from tensorflow.keras.layers import TextVectorization\n",
        "import string\n",
        "import re\n",
        "\n",
        "pattern = \"<[^>]+>\"\n",
        "tf.strings.regex_replace(\"Text with tags.<br /><b>contains html</b>\", pattern, \" \")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sGOLrJOXUMsq",
        "outputId": "795efeea-0730-4cc5-9509-4a0ad9c3296f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(), dtype=string, numpy=b'Text with tags.  contains html '>"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def custom_standardization(input_data):\n",
        "    lowercase = tf.strings.lower(input_data)\n",
        "    stripped_html = tf.strings.regex_replace(lowercase, pattern, \" \")\n",
        "    return tf.strings.regex_replace(stripped_html, f\"[{re.escape(string.punctuation)}]\", \"\")"
      ],
      "metadata": {
        "id": "RRoQJpC3UMvD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model constants\n",
        "\n",
        "max_features = 20000\n",
        "embedding_dim = 128\n",
        "sequence_length = 500"
      ],
      "metadata": {
        "id": "fZCW39R6YHBf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectorize_layer = TextVectorization(\n",
        "    max_tokens = max_features,\n",
        "    standardize = custom_standardization,\n",
        "    output_mode = \"int\",\n",
        "    output_sequence_length = sequence_length\n",
        ")\n",
        "\n",
        "# vocab layer has been created, call `adapt` on a text-only\n",
        "# dataset to create the vocabulary. You don't have to batch, but for very large\n",
        "# datasets this means you're not keeping spare copies of the dataset in memory.\n",
        "\n",
        "text_ds = raw_train_ds.map(lambda x, y: x) # no labels\n",
        "vectorize_layer.adapt(text_ds)"
      ],
      "metadata": {
        "id": "xG9G0H_qZash"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Vectorize the data.\n",
        "def vectorize_text(text, label):\n",
        "    text = tf.expand_dims(text, -1)\n",
        "    return vectorize_layer(text), label\n",
        "\n",
        "\n",
        "train_ds = raw_train_ds.map(vectorize_text)\n",
        "val_ds = raw_val_ds.map(vectorize_text)\n",
        "test_ds = raw_test_ds.map(vectorize_text)\n",
        "\n",
        "# Do async prefetching / buffering of the data for best performance on GPU.\n",
        "train_ds = train_ds.cache().prefetch(buffer_size=10)\n",
        "val_ds = val_ds.cache().prefetch(buffer_size=10)\n",
        "test_ds = test_ds.cache().prefetch(buffer_size=10)"
      ],
      "metadata": {
        "id": "UJrAZXRUZa2a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build a model\n",
        "\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "inputs = layers.Input(shape=(None,), dtype=\"int64\")\n",
        "\n",
        "# Embedding\n",
        "x = layers.Embedding(max_features, embedding_dim)(inputs)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "\n",
        "x = layers.Conv1D(128, 7, padding=\"valid\", activation=\"relu\", strides=3)(x)\n",
        "x = layers.Conv1D(128, 7, padding=\"valid\", activation=\"relu\", strides=3)(x)\n",
        "x = layers.GlobalMaxPooling1D()(x)\n",
        "\n",
        "x = layers.Dense(128, activation=\"relu\")(x)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "\n",
        "predictions = layers.Dense(1, activation=\"sigmoid\", name=\"predictions\")(x)\n",
        "\n",
        "model = tf.keras.Model(inputs, predictions)\n",
        "\n",
        "model.summary()\n",
        "\n",
        "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "djjoeIY7Za4y",
        "outputId": "bb5c5272-a80f-495c-bf5e-bfe7665a0cc1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, None)]            0         \n",
            "                                                                 \n",
            " embedding (Embedding)       (None, None, 128)         2560000   \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, None, 128)         0         \n",
            "                                                                 \n",
            " conv1d (Conv1D)             (None, None, 128)         114816    \n",
            "                                                                 \n",
            " conv1d_1 (Conv1D)           (None, None, 128)         114816    \n",
            "                                                                 \n",
            " global_max_pooling1d (Globa  (None, 128)              0         \n",
            " lMaxPooling1D)                                                  \n",
            "                                                                 \n",
            " dense (Dense)               (None, 128)               16512     \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 128)               0         \n",
            "                                                                 \n",
            " predictions (Dense)         (None, 1)                 129       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2,806,273\n",
            "Trainable params: 2,806,273\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 3\n",
        "\n",
        "model.fit(\n",
        "    train_ds,\n",
        "    validation_data=val_ds,\n",
        "    epochs=epochs\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d6DXUGCbZa7Z",
        "outputId": "e72ce4f3-a2ae-41a8-8e90-bd245b5a31aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "625/625 [==============================] - 21s 20ms/step - loss: 0.5310 - accuracy: 0.6868 - val_loss: 0.3351 - val_accuracy: 0.8542\n",
            "Epoch 2/3\n",
            "625/625 [==============================] - 10s 16ms/step - loss: 0.2338 - accuracy: 0.9064 - val_loss: 0.3074 - val_accuracy: 0.8830\n",
            "Epoch 3/3\n",
            "625/625 [==============================] - 10s 16ms/step - loss: 0.1174 - accuracy: 0.9593 - val_loss: 0.3999 - val_accuracy: 0.8674\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f64e740dfd0>"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.evaluate(test_ds)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2IZf9IdmZa9x",
        "outputId": "505cbdcf-b3b5-49f1-d136-dc8eedb3545a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "782/782 [==============================] - 9s 12ms/step - loss: 0.4473 - accuracy: 0.8464\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.44731128215789795, 0.8464000225067139]"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = tf.keras.Input(shape=(1,), dtype=\"string\")\n",
        "indices = vectorize_layer(inputs)\n",
        "outputs = model(indices)\n",
        "\n",
        "\n",
        "end_to_end_model = tf.keras.Model(inputs, outputs)\n",
        "end_to_end_model.compile(\n",
        "    loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"]\n",
        ")\n",
        "\n",
        "# Test it with `raw_test_ds`, which yields raw strings\n",
        "end_to_end_model.evaluate(raw_test_ds)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "As031FJcZbJy",
        "outputId": "542f9cba-4d4b-441b-c51c-c1ac6af5a994"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "782/782 [==============================] - 10s 12ms/step - loss: 0.4473 - accuracy: 0.8464\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.4473113715648651, 0.8464000225067139]"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    }
  ]
}
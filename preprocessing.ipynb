{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "preprocessing.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyP/R0J392mZKAy5btw46W8+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ithingv/nlp_study/blob/main/preprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#전처리 과정\n",
        "\n",
        "- 코퍼스 수집\n",
        "    1. 공개된 데이터(각종 문제 또는 논문을 위한 데이터, 양이 한정적)\n",
        "    2. 크롤링을 통한 데이터 수집\n",
        "        - 웹 서버 리소스 사용 가능 유무는 robot.txt를 보고 판단\n",
        "        - selnium, bs4, phantomJS, headless brower\n",
        "        - 자동 기계번역을 위한 병렬 코퍼스는 단일 코퍼스에 비해 상당히 어렵다.\n",
        "            - Korean Parallel Data\n",
        "            - https://sites.google.com/site/koreanparalleldata/\n",
        "- 정제\n",
        "    - 전각문자제거\n",
        "        -> 반각문자로 변환\n",
        "    - 대소문자 통일\n",
        "        - 표현을 일원화하여 여러 단어를 하나의 형태로 통일해 희소성을 줄이는 효과\n",
        "        - 단어 임베딩을 통한 효율적인 표현이 가능해지므로 대소문자 통일의 필요성이 줄어들었다.\n",
        "    - 정규표현식 https://regexper.com\n",
        "    - 치환자\n",
        "- 문장 단위 분절\n",
        "    - 텍스트 입력이 문장 단위인 경우가 대부분이다.\n",
        "    - 여러 문장이 한 라인에 있거나, 한 문장이 여러 라인에 걸쳐 있는 경우(문장 합치기)에는 문장 단위 분절이 필요하다.\n",
        "- 분절\n",
        "    - 한국어(Mecab), 일본어(Mecab), 중국어(Jieba)\n",
        "    - 일반적이고 전형적인 쉬운 문장을 처리하는 능력은 비슷\n",
        "    - 고유명사나 신조어를 처리하는 능력은 라이브러리 별로 차이가 있으므로 비교를 잘 해야함\n",
        "- 병렬 코퍼스 정렬(생략가능)\n",
        "- 서브워드 분절\n",
        "    - BPE 알고리즘을 통한 서브워드 단위 분절은 현재 필수 전처리 방법으로 자리잡았다. 서브워드 분절은 기본적으로 '단어는 의미를 가진 더 작은 서브워드들의 조합으로 이루어진다'는 가정하게 적용되는 알고리즘이다.\n",
        "    \n",
        "    - 영어나 한국어 역시 라틴어와 한자를 기반으로 형성된 언어이기에 많은 단어가 서브워드들로 구성된다. 따라서 적절한 서브워드를 발견하여 해당 단위로 쪼개어주면 어휘 수를 줄일 수 있고 **희소성**을 효과적으로 줄일 수 있다.\n",
        "\n",
        "    - ex) concentrate ---> con(=together) + centr(=center) + ate(=make)\n",
        "    - ex) 집중 ---> (모을)집 + (가운데)중\n",
        "\n",
        "    - 서브워드 단위 분절로 얻는 가장 대표적인 효과는 UNK 토큰에 대한 효율적인 대처이다. \n",
        "    - 대부분의 딥러닝 NLP 알고리즘은 입력 문장을 단어들의 시퀀스로 받아들인다. UNK 토큰이 나타나면 이후의 언어 모델의 확률은 크게 망가지고 적절한 문장의 임베딩 또는 생성이 어렵게된다. (이전 단어가 다음 단어가 출현 확률에 영향을 미침)\n",
        "\n",
        "    - 서브워드 단위 분절을 통해 신조어나 오타와 같은 UNK에 대해 서브워드 단위나 문자 단위로 쪼개줌으로써 기존 훈련 데이터에서 보았던 토큰들의 조합으로 만들어버릴 수 있다.\n",
        "    - 즉 UNK 자체를 없앰으로써 효율적으로 UNK에 대처할 수 있다.\n",
        "    - BPE를 적욯할 경우 원래 문장의 띄어쓰기와 BPE로 인한 띄어쓰기를 서로 구분하기 위해 기존 단어 앞에 특수 문자 '_'를 사용한다.(detokenize 할 때 필요)\n",
        "    - 오픈소스\n",
        "        - Sennrich: https://github.com/rsennrich/subword-nmt\n",
        "        - 수정버전: https://github.com/kh-kim/subword-nmt\n",
        "        - SentencePiece: https://github.com/google/sentencepiece\n",
        "        \n"
      ],
      "metadata": {
        "id": "q6nbsXYHzxeH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 문장단위분절 예제\n",
        "\n",
        "import sys, fileinput, re\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "lines = \"자연어처리는 인공지능의 한 줄기 입니다. 시퀀스 투 시퀀스의 등장 이후로 딥러닝을 활용한 자연어처리는 새로운 전기를 맞이하게 되었습니다. 문장을 받아 단순히 수치로 나타내던 시절을 넘어, 원하는대로 문장을 만들어낼 수 있게 된 것입니다.\"\n",
        "nltk.download('punkt')\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    for line in lines:\n",
        "        if line.strip() != \"\":\n",
        "            line = re.sub(r'([a-z])\\.([A-Z])', r'\\1. \\2', line.strip())\n",
        "\n",
        "            sentences = sent_tokenize(line.strip())\n",
        "\n",
        "            for s in sentences:\n",
        "                if s != \"\":\n",
        "                    sys.stdout.write(s + '\\n')"
      ],
      "metadata": {
        "id": "zc5grusv0B3p"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Torchtext\n",
        "\n",
        "- 자연어 처리 문제 또는 텍스트에 관한 머신러닝이나 딥러닝을 수행하는 데이터를 읽고 전처리하는 코드를 모아둔 라이브러리이다. \n",
        "- 예시\n",
        "\n",
        "|x데이터|y데이터| 활용분야|\n",
        "|:-----:|:----:|:---:|\n",
        "| 코퍼스   | 클래스|텍스트 분류, 감성분석|\n",
        "| 코퍼스   | - |언어모델|\n",
        "| 코퍼스   | 코퍼스 | 기계번역, 요약, QuestionAnswering |\n",
        "\n"
      ],
      "metadata": {
        "id": "beVjsCOg24Xi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 코퍼스와 레이블 읽기\n",
        "# 클래스와 텍스트가 탭('\\t')로 구분된 데이터의 입력을 받는 내용\n",
        "# dataloader.py\n",
        "\n",
        "from torchtext import data\n",
        "\n",
        "class DataLoader(object):\n",
        "    def __init__(self, train_fn, valid_fn, \n",
        "                 batch_size=64,\n",
        "                 device=-1,\n",
        "                 max_vocab=999999,\n",
        "                 min_freq=1,\n",
        "                 use_eos=False,\n",
        "                 shuffle=True\n",
        "                 ):\n",
        "        \n",
        "        super(DataLoader, self).__init__()\n",
        "\n",
        "        # Define field of the input file\n",
        "        # The input file consists of two fields\n",
        "        # https://torchtext.readthedocs.io/en/latest/data.html\n",
        "        \n",
        "        self.label = data.Field(sequential=False, # Whether the datatype represents sequential data. If False, no tokenization is applied. Default: True.\n",
        "                                use_vocab=True, # Whether to use a Vocab object. If False, the data in this field should already be numerical. Default: True.\n",
        "                                unk_token=None # unk_token – The string token used to represent OOV words. Default: “<unk>”.\n",
        "                                )\n",
        "        \n",
        "        self.text = data.Field(use_vocab=True,\n",
        "                        batch_first=True,\n",
        "                        include_lengths=False, # Whether to return a tuple of a padded minibatch and a list containing the lengths of each examples, or just a padded minibatch. Default: False.\n",
        "                        eos_token='<EOS>' if use_eos else None # A token that will be appended to every example using this field, or None for no end-of-sentence token. Default: None.\n",
        "                        )\n",
        "\n",
        "        train, valid = data.TabularDataset.splits(path='',\n",
        "                                                  train=train_fn,\n",
        "                                                  validation=valid_fn,\n",
        "                                                  format='tsv',\n",
        "                                                  fields=[('label', self.label),\n",
        "                                                          ('text', self.text)\n",
        "                                                          ]\n",
        "                                                )\n",
        "        \n",
        "        # Those loaded dataset would be feeded into each iterator:\n",
        "        # train iterator and valid iterator\n",
        "        # We sort input sentences by length, to group similar lengths\n",
        "        self.train_iter, self.valid_iter = data.BucketIterator.splits((train, valid),\n",
        "                                                                      batch_size=batch_size,\n",
        "                                                                      device='cuda:%d' % device if device >=0 else 'cpu',\n",
        "                                                                      shuffle=shuffle,\n",
        "                                                                      sort_key=lambda x: len(x.text),\n",
        "                                                                      sort_within_batch=True\n",
        "        )\n",
        "\n",
        "        # At last, we make a vocabulary for label and text field\n",
        "        # It is making mapping table between words and indice\n",
        "        self.label.build_vocab(train)\n",
        "        self.text_build_vocab(train, max_size=max_vocab, min_freq=min_freq)"
      ],
      "metadata": {
        "id": "Ew_kOiOlAEO_"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 코퍼스 읽기\n",
        "    한 라인이 텍스트로만 채워져 있을 때를 위한 코드\n",
        "    주로 언어 모델을 훈련시키는 상황에 쓸 수 있다.\n",
        "    LanguageModel Dataset을 통해 미리 정의된 필드를 텍스트 파일에서 읽어들인다.\n",
        "    이때 각 문장의 길이에 따라 정렬을 통해 비슷한 길이의 문장끼리 미니배치를 만들어준다.\n",
        "    이 작업을 통해 매우 상이한 길이의 문장들이 하나의 미니배치에 묶여 훈련 시간에서 손해보는 것을 방지한다."
      ],
      "metadata": {
        "id": "RYR_EBtHFmGS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 코퍼스와 레이블 읽기\n",
        "# 클래스와 텍스트가 탭('\\t')로 구분된 데이터의 입력을 받는 내용\n",
        "# dataloader.py\n",
        "\n",
        "from torchtext import data, datasets\n",
        "\n",
        "PAD, BOS, EOS = 1, 2, 3\n",
        "\n",
        "class DataLoader(object):\n",
        "    def __init__(self, \n",
        "                 train_fn, \n",
        "                 valid_fn, \n",
        "                 batch_size=64,\n",
        "                 device='cpu',\n",
        "                 max_vocab=999999,\n",
        "                 max_length=255,\n",
        "                 fix_length=None,\n",
        "                 use_bos=True,\n",
        "                 use_eos=True,\n",
        "                 shuffle=True\n",
        "                 ):\n",
        "        \n",
        "        super(DataLoader, self).__init__()\n",
        "\n",
        "        # Define field of the input file\n",
        "        # The input file consists of two fields\n",
        "        # https://torchtext.readthedocs.io/en/latest/data.html\n",
        "        \n",
        "        \n",
        "        self.text = data.Field(sequential=True,\n",
        "                                use_vocab=True,\n",
        "                                batch_first=True,\n",
        "                                include_lengths=True, # Whether to return a tuple of a padded minibatch and a list containing the lengths of each examples, or just a padded minibatch. Default: False.\n",
        "                                fix_length=fix_length,\n",
        "                                init_token='<BOS>' if use_bos else None,\n",
        "                                eos_token='<EOS>' if use_eos else None # A token that will be appended to every example using this field, or None for no end-of-sentence token. Default: None.\n",
        "                                )\n",
        "\n",
        "        train = LanguageModelDataset(path=train_fn,\n",
        "                                     fields=[('text', self.text)],\n",
        "                                     max_length=max_length\n",
        "        )     \n",
        "        \n",
        "        train = LanguageModelDataset(path=valid_fn,\n",
        "                                     fields=[('text', self.text)],\n",
        "                                     max_length=max_length\n",
        "        )     \n",
        "\n",
        "        self.train_iter, self.valid_iter = data.BucketIterator(train,\n",
        "                                                               batch_size=batch_size,\n",
        "                                                               device='cuda:%d' % device if device >=0 else 'cpu',\n",
        "                                                               shuffle=shuffle,\n",
        "                                                               sort_key=lambda x: -len(x.text),\n",
        "                                                                      sort_within_batch=True\n",
        "        )\n",
        "\n",
        "        # At last, we make a vocabulary for label and text field\n",
        "        # It is making mapping table between words and indice\n",
        "        self.label.build_vocab(train)\n",
        "        self.text_build_vocab(train, max_size=max_vocab, min_freq=min_freq)\n",
        "\n",
        "\n",
        "\n",
        "class LanguageModelDataset(data.Dataset):\n",
        "    def __init__(self, path, fields, max_length=None, **kwargs):\n",
        "        if not isinstance(fields[0], (tuple, list)):\n",
        "            fields = [('text', fields[0])]\n",
        "        \n",
        "        examples = []\n",
        "        with open(path) as  f:\n",
        "            for line in f:\n",
        "                line = line.strip()\n",
        "                if max_length and max_length < len(line.split()):\n",
        "                    continue\n",
        "                if line != '':\n",
        "                    examples.append(data.Example.fromlist(\n",
        "                        [line], fields))\n",
        "\n",
        "        super(LanguageModelDataset, self).__init__(examples, fields, **kwargs)"
      ],
      "metadata": {
        "id": "mDutwP7PHQH9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
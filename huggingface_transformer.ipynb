{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "huggingface-transformer.ipynb",
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyMiB5RpjfLDZXqa84WxRjU7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ithingv/nlp_study/blob/main/huggingface_transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9QmTTunUp7j6"
      },
      "outputs": [],
      "source": [
        "!pip install transformers==3"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "・language-modeling(언어 모델)\n",
        "\n",
        "・text-classification(텍스트 분류)\n",
        "\n",
        "・token-classification(고유 표현 추출)\n",
        "\n",
        "・multiple-choice\n",
        "\n",
        "\n",
        "・question-answering(질문 응답)\n",
        "\n",
        "・text-generation(텍스트 생성)\n",
        "\n",
        "・distillation\n",
        "\n",
        "・summarization( 요약)\n",
        "\n",
        "\n",
        "· translation (번역)\n",
        "\n",
        "· bertology\n",
        "\n",
        "· adversarial"
      ],
      "metadata": {
        "id": "ULEzpa1-qEX9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 모델 아키텍처\n",
        "Huggingface Transformers가 제공하는 모델 아키텍처는 다음과 같습니다.\n",
        "\n",
        "(1) ALBERT (Google Research and the Toyota Technological Institute at Chicago) - 2019/9\n",
        "\n",
        "(2) BART (Facebook) - 2019/10\n",
        "\n",
        "(3) BARThez (École polytechnique) -2020/10\n",
        "\n",
        "(4) BERT (Google) - 2018/11\n",
        "\n",
        "\n",
        "(5) BERT For Sequence Generation (Google) - 2019/7\n",
        "\n",
        "(6) Blenderbot (Facebook) - 2020/4\n",
        "\n",
        "(7) BlenderbotSmall (Facebook) - 2020/4\n",
        "\n",
        "(8) CamemBERT (Inria/Facebook/Sorbonne) - 2019/11\n",
        "\n",
        "\n",
        "(9) CTRL (Salesforce) - 2019/9\n",
        "\n",
        "(10) DeBERTa (Microsoft Research) - 2020/6\n",
        "\n",
        "(11) DialoGPT(Microsoft Research) - 2019/11\n",
        "\n",
        "\n",
        "(12) DistilBERT (HuggingFace) - 2019/10\n",
        "\n",
        "(13) DPR (Facebook) - 2020/4\n",
        "\n",
        "\n",
        "(14) ELECTRA (Google Research/Stanford University) - 2020/3\n",
        "\n",
        "(15) FlauBERT ( CNRS) - 2019/12\n",
        "\n",
        "(16) Funnel Transformer (CMU/Google Brain) - 2020/6\n",
        "\n",
        "(17) GPT (OpenAI) - 2018/6\n",
        "\n",
        "\n",
        "(18) GPT-2 (OpenAI) - 2019/2\n",
        "\n",
        "(19) LayoutLM ( Microsoft Research Asia) - 2019/12\n",
        "\n",
        "(20) LED (AllenAI) - 2020/4\n",
        "\n",
        "(21) Longformer (AllenAI) - 2020/4\n",
        "\n",
        "(22) LXMERT(UNC Chapel Hill) - 2019/8\n",
        "\n",
        "(23) MarianMT\n",
        "\n",
        "(24) MBart (Facebook) - 2020/1\n",
        "\n",
        "(25) MPNet (Microsoft Research) - 2020/4\n",
        "\n",
        "(26) MT5 (Google AI) - 2020/10\n",
        "\n",
        "(27 ) Pegasus (Google) - 2019/12\n",
        "\n",
        "(28) ProphetNet (Microsoft Research) -2020/1\n",
        "\n",
        "(29) Reformer (Google Research) - 2020/2\n",
        "(30) RoBERTa (Facebook) - 2019/6\n",
        "\n",
        "(31) SqueezeBert - 2020 /1\n",
        "\n",
        "(32) T5 (Google AI) - 2019/10\n",
        "\n",
        "(33) TAPAS (Google AI) - 2020/4\n",
        "\n",
        "(34) Transformer-XL(Google/CMU) - 2019/1\n",
        "\n",
        "\n",
        "(35) XLM (Facebook) - 2019/1\n",
        "\n",
        "(36) XLM-ProphetNet (Microsoft Research) - 2020/1\n",
        "(37) XLM-RoBERTa (Facebook AI) - 2019/11\n",
        "\n",
        "(38 ) XLNet (Google / CMU) - 2019/6\n"
      ],
      "metadata": {
        "id": "qaPxgdyJqQw5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 사전 학습 모델\n",
        "Huggingface Transformers에서 제공하는 사전 학습된 모델은 아래에서 확인할 수 있습니다.\n",
        "\n",
        "https://huggingface.co/models\n",
        "\n"
      ],
      "metadata": {
        "id": "vzuAYc21qS63"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 9. 파이프라인과 토크나이저\n",
        "\n",
        "Huggingface Transformers는 추론을 수행하기 위해 두 가지 기술을 제공합니다.\n",
        "\n",
        "・파이프라인 : 간단하게 사용할 수 있는(2행으로 구현 가능) 추상화 모델을 제공.\n",
        "\n",
        "· 토크 나이저 : 직접 모델을 조작하여 완전한 추론을 제공."
      ],
      "metadata": {
        "id": "AMOqBcQBqihc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "· feature-extraction : 텍스트를 주면 특징을 나타내는 벡터를 반환합니다.\n",
        "\n",
        "·sentiment-analysis : 텍스트를 주면 감정 분석 결과를 반환합니다.\n",
        "\n",
        "· ner : 텍스트를 주면 고유 표현 추출 결과를 반환합니다.\n",
        "\n",
        "· question-answering : 질문과 기사를 주면 응답을 돌려준다.\n",
        "\n",
        "・fill-mask : 공란이 있는 텍스트를 주면, 공란에 적용되는 단어를 \n",
        "돌려준다.\n",
        "\n",
        "· summarization : 입력한 텍스트를 요약하고 반환합니다.\n",
        "\n",
        "・translation_xx_to_yy : 텍스트를 주면, 번역해 돌려준다.\n",
        "\n",
        "\n",
        "・text-generation : 텍스트를 주면, 그 뒤에 오는 텍스트를 돌려준다.\n",
        "\n",
        "・conversation : 텍스트를 주면, 그 뒤에 오는 대화를 돌려준다."
      ],
      "metadata": {
        "id": "If12oZ44qo7L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 텍스트 분류\n",
        "\" 텍스트 분류 \"는 텍스트를 클래스 분류하는 작업입니다. 데이터 세트의 예는 GLUE 입니다. \n",
        "\n",
        "정밀 튜닝을 원한다면 run_glue.py 또는 run_tf_glue.py 를 사용할 수 있습니다."
      ],
      "metadata": {
        "id": "Xn0cQlFHqo9Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# 감성분석\n",
        "nlp = pipeline(\"sentiment-analysis\")"
      ],
      "metadata": {
        "id": "iKstp1UPq469"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(nlp(\"I love you\"))\n",
        "print(nlp(\"I hate you\"))"
      ],
      "metadata": {
        "id": "72ODZ3Xiq49I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 토크나이저에서 텍스트 분류\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "import torch\n",
        "\n",
        "# 토크나이저, 모델 준비\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased-finetuned-mrpc\")\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-cased-finetuned-mrpc\")\n",
        "\n",
        "# 클래스\n",
        "classes = [\"not paraphrase\", \"is paraphrase\"]\n",
        "\n",
        "sequence_0 = \"The company HuggingFace is based in New York City\"\n",
        "sequence_1 = \"Apples are especially bad for your health\"\n",
        "sequence_2 = \"HuggingFace's headquarters are situated in Manhattan\"\n",
        "\n",
        "# 전처리\n",
        "paraphrase = tokenizer.encode_plus(sequence_0, sequence_2, return_tensors=\"pt\")\n",
        "not_paraphrase = tokenizer.encode_plus(sequence_0, sequence_1, return_tensors=\"pt\")\n",
        "\n",
        "# 추론\n",
        "paraphrase_classification_logits = model(**paraphrase)[0]\n",
        "not_paraphrase_classification_logits = model(**not_paraphrase)[0]\n",
        "\n",
        "# 결과\n",
        "paraphrase_results = torch.softmax(paraphrase_classification_logits, dim=1).tolist()[0]\n",
        "not_paraphrase_results = torch.softmax(not_paraphrase_classification_logits, dim=1).tolist()[0]\n",
        "\n",
        "print(\"Should be paraphrase\")\n",
        "for i in range(len(classes)):\n",
        "  print(f\"{classes[i]}: {round(paraphrase_results[i] * 100)}%\")\n",
        "\n",
        "print(\"\\nShould not be paraphrase\")\n",
        "for i in range(len(classes)):\n",
        "    print(f\"{classes[i]}: {round(not_paraphrase_results[i] * 100)}%\")"
      ],
      "metadata": {
        "id": "bygXDstYq4_g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 질문 응답\n",
        "\n",
        "질문 응답 '은 주어진 텍스트에서 질문 응답을 추출하는 작업입니다. 데이터 세트의 예는 SQuAD 입니다. \n",
        "\n",
        "파인 튜닝하고 싶은 경우는 「run_squad.py」를 이용할 수 있습니다."
      ],
      "metadata": {
        "id": "k5dZihE8q5B6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# 파이프라인에서 \"질문 응답\"을 수행하는 예시\n",
        "nlp = pipeline(\"question-answering\")\n",
        "\n",
        "# 텍스트\n",
        "context = r\"\"\"\n",
        "Extractive Question Answering is the task of extracting an answer from a text given a question. An example of a\n",
        "question answering dataset is the SQuAD dataset, which is entirely based on that task. If you would like to fine-tune\n",
        "a model on a SQuAD task, you may leverage the `run_squad.py`.\n",
        "\"\"\"\n",
        "\n",
        "# 추론\n",
        "print(nlp(question=\"What is extractive question answering?\", context=context))\n",
        "print(nlp(question=\"What is a good example of a question answering dataset?\", context=context))"
      ],
      "metadata": {
        "id": "Hkas06b6q5EK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  토크 나이저에서 \"질문 응답\"을 수행하는 예입니다."
      ],
      "metadata": {
        "id": "wb3WUQ4lq5GV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
        "import torch\n",
        "\n",
        "# 토크나이저 모델 \n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-large-uncased-whole-word-masking-finetuned-squad\")\n",
        "model = AutoModelForQuestionAnswering.from_pretrained(\"bert-large-uncased-whole-word-masking-finetuned-squad\")\n",
        "\n",
        "# 텍스트\n",
        "text = r\"\"\"\n",
        "🤗 Transformers (formerly known as pytorch-transformers and pytorch-pretrained-bert) provides general-purpose\n",
        "architectures (BERT, GPT-2, RoBERTa, XLM, DistilBert, XLNet…) for Natural Language Understanding (NLU) and Natural\n",
        "Language Generation (NLG) with over 32+ pretrained models in 100+ languages and deep interoperability between\n",
        "TensorFlow 2.0 and PyTorch.\n",
        "\"\"\"\n",
        "\n",
        "# 질문\n",
        "questions = [\n",
        "    \"How many pretrained models are available in Transformers?\",\n",
        "    \"What does Transformers provide?\",\n",
        "    \"Transformers provides interoperability between which frameworks?\",\n",
        "]\n",
        "\n",
        "for question in questions:\n",
        "    # 전처리\n",
        "    inputs = tokenizer.encode_plus(question, text, add_special_tokens=True, return_tensors=\"pt\")\n",
        "    input_ids = inputs[\"input_ids\"].tolist()[0]\n",
        "\n",
        "    # 추론\n",
        "    text_tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
        "    answer_start_scores, answer_end_scores = model(**inputs)\n",
        "\n",
        "    print( answer_start_scores, answer_end_scores )\n",
        "    # Likelihood가 높은 응답값 \n",
        "    answer_start = torch.argmax(answer_start_scores)  \n",
        "    answer_end = torch.argmax(answer_end_scores) + 1 \n",
        "    answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(input_ids[answer_start:answer_end]))\n",
        "\n",
        "    # 추출\n",
        "    print(f\"Question: {question}\")\n",
        "    print(f\"Answer: {answer}\\n\")"
      ],
      "metadata": {
        "id": "LlZHgeFVq5KH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "ZsYEa_XztumQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
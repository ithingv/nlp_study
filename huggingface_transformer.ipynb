{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "huggingface-transformer.ipynb",
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyMiB5RpjfLDZXqa84WxRjU7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ithingv/nlp_study/blob/main/huggingface_transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9QmTTunUp7j6"
      },
      "outputs": [],
      "source": [
        "!pip install transformers==3"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ãƒ»language-modeling(ì–¸ì–´ ëª¨ë¸)\n",
        "\n",
        "ãƒ»text-classification(í…ìŠ¤íŠ¸ ë¶„ë¥˜)\n",
        "\n",
        "ãƒ»token-classification(ê³ ìœ  í‘œí˜„ ì¶”ì¶œ)\n",
        "\n",
        "ãƒ»multiple-choice\n",
        "\n",
        "\n",
        "ãƒ»question-answering(ì§ˆë¬¸ ì‘ë‹µ)\n",
        "\n",
        "ãƒ»text-generation(í…ìŠ¤íŠ¸ ìƒì„±)\n",
        "\n",
        "ãƒ»distillation\n",
        "\n",
        "ãƒ»summarization( ìš”ì•½)\n",
        "\n",
        "\n",
        "Â· translation (ë²ˆì—­)\n",
        "\n",
        "Â· bertology\n",
        "\n",
        "Â· adversarial"
      ],
      "metadata": {
        "id": "ULEzpa1-qEX9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ëª¨ë¸ ì•„í‚¤í…ì²˜\n",
        "Huggingface Transformersê°€ ì œê³µí•˜ëŠ” ëª¨ë¸ ì•„í‚¤í…ì²˜ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.\n",
        "\n",
        "(1) ALBERT (Google Research and the Toyota Technological Institute at Chicago) - 2019/9\n",
        "\n",
        "(2) BART (Facebook) - 2019/10\n",
        "\n",
        "(3) BARThez (Ã‰cole polytechnique) -2020/10\n",
        "\n",
        "(4) BERT (Google) - 2018/11\n",
        "\n",
        "\n",
        "(5) BERT For Sequence Generation (Google) - 2019/7\n",
        "\n",
        "(6) Blenderbot (Facebook) - 2020/4\n",
        "\n",
        "(7) BlenderbotSmall (Facebook) - 2020/4\n",
        "\n",
        "(8) CamemBERT (Inria/Facebook/Sorbonne) - 2019/11\n",
        "\n",
        "\n",
        "(9) CTRL (Salesforce) - 2019/9\n",
        "\n",
        "(10) DeBERTa (Microsoft Research) - 2020/6\n",
        "\n",
        "(11) DialoGPT(Microsoft Research) - 2019/11\n",
        "\n",
        "\n",
        "(12) DistilBERT (HuggingFace) - 2019/10\n",
        "\n",
        "(13) DPR (Facebook) - 2020/4\n",
        "\n",
        "\n",
        "(14) ELECTRA (Google Research/Stanford University) - 2020/3\n",
        "\n",
        "(15) FlauBERT ( CNRS) - 2019/12\n",
        "\n",
        "(16) Funnel Transformer (CMU/Google Brain) - 2020/6\n",
        "\n",
        "(17) GPT (OpenAI) - 2018/6\n",
        "\n",
        "\n",
        "(18) GPT-2 (OpenAI) - 2019/2\n",
        "\n",
        "(19) LayoutLM ( Microsoft Research Asia) - 2019/12\n",
        "\n",
        "(20) LED (AllenAI) - 2020/4\n",
        "\n",
        "(21) Longformer (AllenAI) - 2020/4\n",
        "\n",
        "(22) LXMERT(UNC Chapel Hill) - 2019/8\n",
        "\n",
        "(23) MarianMT\n",
        "\n",
        "(24) MBart (Facebook) - 2020/1\n",
        "\n",
        "(25) MPNet (Microsoft Research) - 2020/4\n",
        "\n",
        "(26) MT5 (Google AI) - 2020/10\n",
        "\n",
        "(27 ) Pegasus (Google) - 2019/12\n",
        "\n",
        "(28) ProphetNet (Microsoft Research) -2020/1\n",
        "\n",
        "(29) Reformer (Google Research) - 2020/2\n",
        "(30) RoBERTa (Facebook) - 2019/6\n",
        "\n",
        "(31) SqueezeBert - 2020 /1\n",
        "\n",
        "(32) T5 (Google AI) - 2019/10\n",
        "\n",
        "(33) TAPAS (Google AI) - 2020/4\n",
        "\n",
        "(34) Transformer-XL(Google/CMU) - 2019/1\n",
        "\n",
        "\n",
        "(35) XLM (Facebook) - 2019/1\n",
        "\n",
        "(36) XLM-ProphetNet (Microsoft Research) - 2020/1\n",
        "(37) XLM-RoBERTa (Facebook AI) - 2019/11\n",
        "\n",
        "(38 ) XLNet (Google / CMU) - 2019/6\n"
      ],
      "metadata": {
        "id": "qaPxgdyJqQw5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ì‚¬ì „ í•™ìŠµ ëª¨ë¸\n",
        "Huggingface Transformersì—ì„œ ì œê³µí•˜ëŠ” ì‚¬ì „ í•™ìŠµëœ ëª¨ë¸ì€ ì•„ë˜ì—ì„œ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
        "\n",
        "https://huggingface.co/models\n",
        "\n"
      ],
      "metadata": {
        "id": "vzuAYc21qS63"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 9. íŒŒì´í”„ë¼ì¸ê³¼ í† í¬ë‚˜ì´ì €\n",
        "\n",
        "Huggingface TransformersëŠ” ì¶”ë¡ ì„ ìˆ˜í–‰í•˜ê¸° ìœ„í•´ ë‘ ê°€ì§€ ê¸°ìˆ ì„ ì œê³µí•©ë‹ˆë‹¤.\n",
        "\n",
        "ãƒ»íŒŒì´í”„ë¼ì¸ : ê°„ë‹¨í•˜ê²Œ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ”(2í–‰ìœ¼ë¡œ êµ¬í˜„ ê°€ëŠ¥) ì¶”ìƒí™” ëª¨ë¸ì„ ì œê³µ.\n",
        "\n",
        "Â· í† í¬ ë‚˜ì´ì € : ì§ì ‘ ëª¨ë¸ì„ ì¡°ì‘í•˜ì—¬ ì™„ì „í•œ ì¶”ë¡ ì„ ì œê³µ."
      ],
      "metadata": {
        "id": "AMOqBcQBqihc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Â· feature-extraction : í…ìŠ¤íŠ¸ë¥¼ ì£¼ë©´ íŠ¹ì§•ì„ ë‚˜íƒ€ë‚´ëŠ” ë²¡í„°ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.\n",
        "\n",
        "Â·sentiment-analysis : í…ìŠ¤íŠ¸ë¥¼ ì£¼ë©´ ê°ì • ë¶„ì„ ê²°ê³¼ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.\n",
        "\n",
        "Â· ner : í…ìŠ¤íŠ¸ë¥¼ ì£¼ë©´ ê³ ìœ  í‘œí˜„ ì¶”ì¶œ ê²°ê³¼ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.\n",
        "\n",
        "Â· question-answering : ì§ˆë¬¸ê³¼ ê¸°ì‚¬ë¥¼ ì£¼ë©´ ì‘ë‹µì„ ëŒë ¤ì¤€ë‹¤.\n",
        "\n",
        "ãƒ»fill-mask : ê³µë€ì´ ìˆëŠ” í…ìŠ¤íŠ¸ë¥¼ ì£¼ë©´, ê³µë€ì— ì ìš©ë˜ëŠ” ë‹¨ì–´ë¥¼ \n",
        "ëŒë ¤ì¤€ë‹¤.\n",
        "\n",
        "Â· summarization : ì…ë ¥í•œ í…ìŠ¤íŠ¸ë¥¼ ìš”ì•½í•˜ê³  ë°˜í™˜í•©ë‹ˆë‹¤.\n",
        "\n",
        "ãƒ»translation_xx_to_yy : í…ìŠ¤íŠ¸ë¥¼ ì£¼ë©´, ë²ˆì—­í•´ ëŒë ¤ì¤€ë‹¤.\n",
        "\n",
        "\n",
        "ãƒ»text-generation : í…ìŠ¤íŠ¸ë¥¼ ì£¼ë©´, ê·¸ ë’¤ì— ì˜¤ëŠ” í…ìŠ¤íŠ¸ë¥¼ ëŒë ¤ì¤€ë‹¤.\n",
        "\n",
        "ãƒ»conversation : í…ìŠ¤íŠ¸ë¥¼ ì£¼ë©´, ê·¸ ë’¤ì— ì˜¤ëŠ” ëŒ€í™”ë¥¼ ëŒë ¤ì¤€ë‹¤."
      ],
      "metadata": {
        "id": "If12oZ44qo7L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# í…ìŠ¤íŠ¸ ë¶„ë¥˜\n",
        "\" í…ìŠ¤íŠ¸ ë¶„ë¥˜ \"ëŠ” í…ìŠ¤íŠ¸ë¥¼ í´ë˜ìŠ¤ ë¶„ë¥˜í•˜ëŠ” ì‘ì—…ì…ë‹ˆë‹¤. ë°ì´í„° ì„¸íŠ¸ì˜ ì˜ˆëŠ” GLUE ì…ë‹ˆë‹¤. \n",
        "\n",
        "ì •ë°€ íŠœë‹ì„ ì›í•œë‹¤ë©´ run_glue.py ë˜ëŠ” run_tf_glue.py ë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
      ],
      "metadata": {
        "id": "Xn0cQlFHqo9Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# ê°ì„±ë¶„ì„\n",
        "nlp = pipeline(\"sentiment-analysis\")"
      ],
      "metadata": {
        "id": "iKstp1UPq469"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(nlp(\"I love you\"))\n",
        "print(nlp(\"I hate you\"))"
      ],
      "metadata": {
        "id": "72ODZ3Xiq49I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# í† í¬ë‚˜ì´ì €ì—ì„œ í…ìŠ¤íŠ¸ ë¶„ë¥˜\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "import torch\n",
        "\n",
        "# í† í¬ë‚˜ì´ì €, ëª¨ë¸ ì¤€ë¹„\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased-finetuned-mrpc\")\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-cased-finetuned-mrpc\")\n",
        "\n",
        "# í´ë˜ìŠ¤\n",
        "classes = [\"not paraphrase\", \"is paraphrase\"]\n",
        "\n",
        "sequence_0 = \"The company HuggingFace is based in New York City\"\n",
        "sequence_1 = \"Apples are especially bad for your health\"\n",
        "sequence_2 = \"HuggingFace's headquarters are situated in Manhattan\"\n",
        "\n",
        "# ì „ì²˜ë¦¬\n",
        "paraphrase = tokenizer.encode_plus(sequence_0, sequence_2, return_tensors=\"pt\")\n",
        "not_paraphrase = tokenizer.encode_plus(sequence_0, sequence_1, return_tensors=\"pt\")\n",
        "\n",
        "# ì¶”ë¡ \n",
        "paraphrase_classification_logits = model(**paraphrase)[0]\n",
        "not_paraphrase_classification_logits = model(**not_paraphrase)[0]\n",
        "\n",
        "# ê²°ê³¼\n",
        "paraphrase_results = torch.softmax(paraphrase_classification_logits, dim=1).tolist()[0]\n",
        "not_paraphrase_results = torch.softmax(not_paraphrase_classification_logits, dim=1).tolist()[0]\n",
        "\n",
        "print(\"Should be paraphrase\")\n",
        "for i in range(len(classes)):\n",
        "  print(f\"{classes[i]}: {round(paraphrase_results[i] * 100)}%\")\n",
        "\n",
        "print(\"\\nShould not be paraphrase\")\n",
        "for i in range(len(classes)):\n",
        "    print(f\"{classes[i]}: {round(not_paraphrase_results[i] * 100)}%\")"
      ],
      "metadata": {
        "id": "bygXDstYq4_g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ì§ˆë¬¸ ì‘ë‹µ\n",
        "\n",
        "ì§ˆë¬¸ ì‘ë‹µ 'ì€ ì£¼ì–´ì§„ í…ìŠ¤íŠ¸ì—ì„œ ì§ˆë¬¸ ì‘ë‹µì„ ì¶”ì¶œí•˜ëŠ” ì‘ì—…ì…ë‹ˆë‹¤. ë°ì´í„° ì„¸íŠ¸ì˜ ì˜ˆëŠ” SQuAD ì…ë‹ˆë‹¤. \n",
        "\n",
        "íŒŒì¸ íŠœë‹í•˜ê³  ì‹¶ì€ ê²½ìš°ëŠ” ã€Œrun_squad.pyã€ë¥¼ ì´ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
      ],
      "metadata": {
        "id": "k5dZihE8q5B6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# íŒŒì´í”„ë¼ì¸ì—ì„œ \"ì§ˆë¬¸ ì‘ë‹µ\"ì„ ìˆ˜í–‰í•˜ëŠ” ì˜ˆì‹œ\n",
        "nlp = pipeline(\"question-answering\")\n",
        "\n",
        "# í…ìŠ¤íŠ¸\n",
        "context = r\"\"\"\n",
        "Extractive Question Answering is the task of extracting an answer from a text given a question. An example of a\n",
        "question answering dataset is the SQuAD dataset, which is entirely based on that task. If you would like to fine-tune\n",
        "a model on a SQuAD task, you may leverage the `run_squad.py`.\n",
        "\"\"\"\n",
        "\n",
        "# ì¶”ë¡ \n",
        "print(nlp(question=\"What is extractive question answering?\", context=context))\n",
        "print(nlp(question=\"What is a good example of a question answering dataset?\", context=context))"
      ],
      "metadata": {
        "id": "Hkas06b6q5EK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  í† í¬ ë‚˜ì´ì €ì—ì„œ \"ì§ˆë¬¸ ì‘ë‹µ\"ì„ ìˆ˜í–‰í•˜ëŠ” ì˜ˆì…ë‹ˆë‹¤."
      ],
      "metadata": {
        "id": "wb3WUQ4lq5GV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
        "import torch\n",
        "\n",
        "# í† í¬ë‚˜ì´ì € ëª¨ë¸ \n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-large-uncased-whole-word-masking-finetuned-squad\")\n",
        "model = AutoModelForQuestionAnswering.from_pretrained(\"bert-large-uncased-whole-word-masking-finetuned-squad\")\n",
        "\n",
        "# í…ìŠ¤íŠ¸\n",
        "text = r\"\"\"\n",
        "ğŸ¤— Transformers (formerly known as pytorch-transformers and pytorch-pretrained-bert) provides general-purpose\n",
        "architectures (BERT, GPT-2, RoBERTa, XLM, DistilBert, XLNetâ€¦) for Natural Language Understanding (NLU) and Natural\n",
        "Language Generation (NLG) with over 32+ pretrained models in 100+ languages and deep interoperability between\n",
        "TensorFlow 2.0 and PyTorch.\n",
        "\"\"\"\n",
        "\n",
        "# ì§ˆë¬¸\n",
        "questions = [\n",
        "    \"How many pretrained models are available in Transformers?\",\n",
        "    \"What does Transformers provide?\",\n",
        "    \"Transformers provides interoperability between which frameworks?\",\n",
        "]\n",
        "\n",
        "for question in questions:\n",
        "    # ì „ì²˜ë¦¬\n",
        "    inputs = tokenizer.encode_plus(question, text, add_special_tokens=True, return_tensors=\"pt\")\n",
        "    input_ids = inputs[\"input_ids\"].tolist()[0]\n",
        "\n",
        "    # ì¶”ë¡ \n",
        "    text_tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
        "    answer_start_scores, answer_end_scores = model(**inputs)\n",
        "\n",
        "    print( answer_start_scores, answer_end_scores )\n",
        "    # Likelihoodê°€ ë†’ì€ ì‘ë‹µê°’ \n",
        "    answer_start = torch.argmax(answer_start_scores)  \n",
        "    answer_end = torch.argmax(answer_end_scores) + 1 \n",
        "    answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(input_ids[answer_start:answer_end]))\n",
        "\n",
        "    # ì¶”ì¶œ\n",
        "    print(f\"Question: {question}\")\n",
        "    print(f\"Answer: {answer}\\n\")"
      ],
      "metadata": {
        "id": "LlZHgeFVq5KH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "ZsYEa_XztumQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}